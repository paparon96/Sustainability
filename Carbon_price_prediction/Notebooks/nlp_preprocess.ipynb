{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/\n",
        "# https://colab.research.google.com/github/gal-a/blog/blob/master/docs/notebooks/nlp/nltk_preprocess.ipynb"
      ],
      "metadata": {
        "id": "wetPcuiTTwZk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "_ZVLsqmJAcZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if running in Google Collab\n",
        "# Mount google drive if running from Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set current directory if running from Google Collab\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Carbon_price_prediction/Workspace/Data')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ijxHd92TnUY",
        "outputId": "eac1d29e-5b1e-44d5-df47-bef89dabe1b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom functions"
      ],
      "metadata": {
        "id": "E2SYCbr0BHY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove accents function\n",
        "def remove_accents(data):\n",
        "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")"
      ],
      "metadata": {
        "id": "bLgn-i6HBJUl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters / Constants"
      ],
      "metadata": {
        "id": "iL7Dq1GUAe4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "# POS (Parts Of Speech) for: nouns, adjectives, verbs and adverbs\n",
        "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} \n",
        "POS_TYPES = list(DI_POS_TYPES.keys())\n",
        "\n",
        "# Constraints on tokens\n",
        "MIN_STR_LEN = 3\n",
        "RE_VALID = '[a-zA-Z]'"
      ],
      "metadata": {
        "id": "0x9EpJqFDhOm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data import"
      ],
      "metadata": {
        "id": "_V32IcvVAr8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = pd.read_csv( \"./no_keyword_merged_articles.csv\", index_col=0)\n",
        "raw_text.head()"
      ],
      "metadata": {
        "id": "YYfmBaNVAvTl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7e87417d-b101-4829-ff6d-cf2fefbd2ff8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date                                               text\n",
              "0  2017-01-01  conceit every generation believe experience un...\n",
              "1  2017-01-01  2016 comes close world leaders appear eager st...\n",
              "2  2017-01-01  process automatic browser redirect requested c...\n",
              "3  2017-01-01  labour’s divisions immigration broken party’s ...\n",
              "4  2017-01-01  established political order came crashing grou..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23bed517-4b50-45c1-848f-14400a513a74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>conceit every generation believe experience un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2016 comes close world leaders appear eager st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>process automatic browser redirect requested c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>labour’s divisions immigration broken party’s ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>established political order came crashing grou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23bed517-4b50-45c1-848f-14400a513a74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-23bed517-4b50-45c1-848f-14400a513a74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-23bed517-4b50-45c1-848f-14400a513a74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text.shape"
      ],
      "metadata": {
        "id": "_yL-tl_PRFzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9a6491-899d-4845-dc94-3e918c71afbd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18939, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame columns to list of tuples\n",
        "raw_text_iter = list(zip(raw_text.date, raw_text.text))"
      ],
      "metadata": {
        "id": "ISgGFo4GRic0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_text_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q74SKiChRtKx",
        "outputId": "d18048f5-6903-44e0-87c5-6f2728fdab04"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18939"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %time\n",
        "# # Check if ETS 3-gram keywords are really not present\n",
        "# keywords = ['European Trading System', 'European Trading Scheme',\n",
        "#             'Emissions Trading System', 'Emissions Trading Scheme']\n",
        "\n",
        "# keyword_matches = {}\n",
        "# for keyword in keywords:\n",
        "#     keyword_matches[keyword] = raw_text.text.str.contains(keyword, case=False).sum()\n",
        "\n",
        "# print(keyword_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkXFtefHVCcP",
        "outputId": "43f8d238-0ec9-46b7-c132-d870a4aa7604"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 5.96 µs\n",
            "{'European Trading System': 0, 'European Trading Scheme': 0, 'Emissions Trading System': 26, 'Emissions Trading Scheme': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test 3-gram keywords\n",
        "# raw_text_iter = [(1, 'test European Trading System right'),\n",
        "#                  (1, 'test European Trading Scheme right'),\n",
        "#                  (1, 'test Emissions Trading System right'),\n",
        "#                  (1, 'test Emissions Trading Scheme right'),]"
      ],
      "metadata": {
        "id": "pMH_K6iwNSvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP preprocessing"
      ],
      "metadata": {
        "id": "YW-dfTY2AtiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "K_kDwUpeTTCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time # Jupyter notebook magic does not work as expected for some reason (shows negligible time)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Process all article texts\n",
        "lemmatized_results = []\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for date, text in raw_text_iter:\n",
        "\n",
        "    if counter % 1000 == 0:\n",
        "        print(f\"Iteration: {counter + 1}/{len(raw_text_iter)}\")\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        continue\n",
        "    # Tokenize by sentence, then by lowercase word\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "\n",
        "    lemmas = []\n",
        "    # Process all tokens per article text\n",
        "    for token in tokens:\n",
        "        # Remove accents\n",
        "        t = remove_accents(token)\n",
        "\n",
        "        # Remove punctuation\n",
        "        t = str(t).translate(string.punctuation)\n",
        "        \n",
        "        # Add token that represents \"no lemmatization match\"\n",
        "        lemmas.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
        "\n",
        "        # Process each token\n",
        "        if t not in stopwords:\n",
        "            if re.search(RE_VALID, t):\n",
        "                if len(t) >= MIN_STR_LEN:\n",
        "                    # Note that the POS (Part Of Speech) is necessary as input to the lemmatizer \n",
        "                    # (otherwise it assumes the word is a noun)\n",
        "                    pos = nltk.pos_tag([t])[0][1][:2]\n",
        "                    pos2 = 'n'  # set default to noun\n",
        "                    if pos in DI_POS_TYPES:\n",
        "                      pos2 = DI_POS_TYPES[pos]\n",
        "                    \n",
        "                    stem = stemmer.stem(t)\n",
        "                    lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
        "                    \n",
        "                    if pos in POS_TYPES:\n",
        "                        # Remove the \"-\" token and append the lemmatization match\n",
        "                        lemmas = lemmas[:-1] \n",
        "                        lemmas.append(lem)\n",
        "    \n",
        "    # Build list of strings from lemmatized tokens\n",
        "    str_lemmas = ' '.join(lemmas)\n",
        "    lemmatized_results.append((date, str_lemmas))\n",
        "    \n",
        "    # Increment counter\n",
        "    counter += 1\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print('Code execution took', round(end-start, 2), 'seconds.')"
      ],
      "metadata": {
        "id": "rAByDwKITjga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d12596-8dc0-4440-f449-5190b842fefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
            "Wall time: 5.48 µs\n",
            "Iteration: 1/4\n",
            "Code execution took 0.01 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text_df = pd.DataFrame(lemmatized_results)\n",
        "lemmatized_text_df.columns = ['date', 'lemmatized_text']\n",
        "\n",
        "print(lemmatized_text_df.shape)\n",
        "print(lemmatized_text_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlVdIkEDU0w",
        "outputId": "42e23fff-187e-4349-a99a-766bd12f89ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 2)\n",
            "   date                     lemmatized_text\n",
            "0     1  test european trading system right\n",
            "1     1  test european trading scheme right\n",
            "2     1  test emission trading system right\n",
            "3     1  test emission trading scheme right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export results"
      ],
      "metadata": {
        "id": "HWZNgi2ABozs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV or pickle should be used?? --> depends on the final format, decide once preproc workflow is complete!\n",
        "lemmatized_text_df.to_csv(f'./lemmatized_merged_articles.csv')\n",
        "# preprocessed_text_df.to_csv(f'./lemmatized_merged_articles.csv')"
      ],
      "metadata": {
        "id": "M7r4nHNYBp9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store data (serialize)\n",
        "with open('lemmatized_merged_articles_{}.pkl'.format(MIN_STR_LEN), 'wb') as handle:\n",
        "   pickle.dump(lemmatized_results, handle)"
      ],
      "metadata": {
        "id": "RzW7SRfACez_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyeCINJ5DGvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support"
      ],
      "metadata": {
        "id": "fI2kyAXsPQEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlRTRLPJTTbP"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"\"\"deforestation forestation emission european trading system solar power nox ccus photovoltaic aren't\"\"\"\n",
        "test = raw_text.text.iloc[0]\n",
        "\n",
        "lemmatize_word(test)[:5]"
      ],
      "metadata": {
        "id": "emvB13VZTcZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}